{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ed9dee-8f38-458c-a778-3099240fbf47",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS FOR EACH OF THE FOUR GROUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fa1865-b058-44ed-874a-e346f7919e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\danie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\danie/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place IDs: {'Sudwerk Brewing Co': 'ChIJr3q7iXsphYARYLX7wjaltRY', 'Open Rice Kitchen': 'ChIJZSAxmnMphYAR8GxaL7kFMTU', 'Davis Noodle City': 'ChIJg4FQP3MphYAR8wwf5EwrMBo', 'Cloud Forest Cafe': 'ChIJq21S-j8ohYARQfzVhPtdoek', 'Ali Baba': 'ChIJC_ykvQsphYARh3oRZ0rSe-0', 'Lazi Cow - Davis': 'ChIJv8H__HQphYARdw_pV20VYMk', 'El Patio Fresh Mexican Grill': 'ChIJh3gTnK8phYARE_gW3fvYq1A', 'Ding How Restaurant': 'ChIJmcBpWrYphYARpGLpeDgH0yY', 'i-Tea': 'ChIJxalQIG0phYARYgLMQ7tFCyk', 'Teabo Cafe': 'ChIJw-M_62MphYARKD_qFV4eFso', 'Dos Coyotes Border Cafe': 'ChIJdX32DmQphYARp9WOii6MZw0', \"Mr. Pickle's Sandwich Shop\": 'ChIJ9_dUk2MphYARv1J8ms7sDDw', 'Pizza Guys': 'ChIJT9yzWnYphYARtbZFkHkxFi8', \"Cenario's Pizza of Davis\": 'ChIJK8OAk5AphYARHDoFj8gLL7U', 'ASUCD Coffee House Coho': 'ChIJK0oJGg8phYARUPGTT8ClXTU'}\n",
      "Reviews saved\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "import requests\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "key = \"AIzaSyBVuUp1tjagr2ghi3TXTgf3GnLIzRBxIDc\"  \n",
    "\n",
    "mod_dollar_restaurants = [\n",
    "    \"Sudwerk Brewing Co\",\n",
    "    \"Open Rice Kitchen\",\n",
    "    \"Davis Noodle City\",\n",
    "    \"Cloud Forest Cafe\",\n",
    "    \"Ali Baba\",\n",
    "    \"Lazi Cow - Davis\",\n",
    "    \"El Patio Fresh Mexican Grill\",\n",
    "    \"Ding How Restaurant\",\n",
    "    \"i-Tea\",\n",
    "    \"Teabo Cafe\",\n",
    "    \"Dos Coyotes Border Cafe\",\n",
    "    \"Mr. Pickle's Sandwich Shop\",\n",
    "    \"Pizza Guys\",\n",
    "    \"Cenario's Pizza of Davis\",\n",
    "    \"ASUCD Coffee House Coho\",\n",
    "]\n",
    "\n",
    "place_ids = {}\n",
    "\n",
    "for restaurant in mod_dollar_restaurants:\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json\"\n",
    "    params = {\n",
    "        \"input\": restaurant,\n",
    "        \"inputtype\": \"textquery\",\n",
    "        \"fields\": \"place_id\",\n",
    "        \"key\": key\n",
    "    }\n",
    "    response = requests.get(url, params=params).json()\n",
    "\n",
    "    if response.get(\"candidates\"):\n",
    "        place_id = response[\"candidates\"][0][\"place_id\"]\n",
    "        place_ids[restaurant] = place_id\n",
    "    else:\n",
    "        print(f\"No results found for {restaurant}\")\n",
    "\n",
    "print(\"Place IDs:\", place_ids)\n",
    "\n",
    "def clean_review(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  \n",
    "    words = word_tokenize(text)  \n",
    "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  \n",
    "    return \" \".join(cleaned_words)  \n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity # analyzes sentiment of given reviews\n",
    "    \n",
    "    if polarity > 0:\n",
    "        sentiment = \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    \n",
    "    return polarity, sentiment\n",
    "\n",
    "def get_reviews(place_id):\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "    params = {\n",
    "        \"place_id\": place_id,\n",
    "        \"fields\": \"name,rating,reviews\",\n",
    "        \"key\": key\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.encoding = \"utf-8\"  \n",
    "    data = response.json()  \n",
    "\n",
    "    reviews_data = []\n",
    "    sentiment_counts = {\"Positive\": 0, \"Neutral\": 0, \"Negative\": 0} # 3 different types of sentiment\n",
    "    \n",
    "    if \"result\" in data:\n",
    "        name = data[\"result\"].get(\"name\", \"N/A\")\n",
    "        overall_rating = data[\"result\"].get(\"rating\", \"N/A\")\n",
    "        reviews = data[\"result\"].get(\"reviews\", [])\n",
    "\n",
    "        for review in reviews:\n",
    "            raw_text = review.get(\"text\", \"No review text available\")\n",
    "            cleaned_text = clean_review(raw_text)\n",
    "            review_rating = review.get(\"rating\", \"N/A\")\n",
    "\n",
    "            \n",
    "            polarity, sentiment = analyze_sentiment(cleaned_text)\n",
    "            sentiment_counts[sentiment] += 1\n",
    "\n",
    "            reviews_data.append([name, overall_rating, cleaned_text, review_rating, polarity, sentiment])\n",
    "\n",
    "    \n",
    "    total_reviews = sum(sentiment_counts.values())\n",
    "    if total_reviews > 0: # this calculates the percentages of each restaurant's sentiment\n",
    "        positive_pct = (sentiment_counts[\"Positive\"] / total_reviews) * 100\n",
    "        neutral_pct = (sentiment_counts[\"Neutral\"] / total_reviews) * 100\n",
    "        negative_pct = (sentiment_counts[\"Negative\"] / total_reviews) * 100\n",
    "    else:\n",
    "        positive_pct = neutral_pct = negative_pct = 0\n",
    "\n",
    "    return reviews_data, positive_pct, neutral_pct, negative_pct\n",
    "\n",
    "\n",
    "with open(\"Google_mod_dollar_Sentiment.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8-sig\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Restaurant\", \"Overall Rating\", \"Cleaned Review\", \"Review Rating\", \"Sentiment Score\", \"Sentiment\", \n",
    "                     \"Positive %\", \"Neutral %\", \"Negative %\"])\n",
    "\n",
    "    for restaurant, place_id in place_ids.items():\n",
    "        reviews, pos_pct, neu_pct, neg_pct = get_reviews(place_id)\n",
    "        \n",
    "        for review in reviews:\n",
    "            writer.writerow(review + [pos_pct, neu_pct, neg_pct])\n",
    "\n",
    "print(\"Reviews saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e807b506-886b-4142-970f-425c58151a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\danie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\danie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place IDs: {\"Bull 'N Mouth\": 'ChIJ2cjfmBAphYARfUYjtzfuBLU', 'Yuchan Shokudo': 'ChIJW3-_PXMphYARJeffp8JfI54', 'Paesanos': 'ChIJkUhmonMphYARMcwrjCo4fgs', 'Mamma Eatery': 'ChIJiSuiDXcphYARZdGfamVWKL4', 'Dumpling House': 'ChIJKeF15VUphYARzXTBAB_pXoM', 'Bruxie': 'ChIJNew_HgAphYARl948GCCYXCs', 'Burgers and Brew': 'ChIJu9RCPwsphYARP6M9AUNgJoI', 'Cafe Bernardo ': 'ChIJD4SWrXQphYARB8769eSqt2E', 'Well Season': 'ChIJX1rq1DYphYAR5TA-v_Vzdbw', 'Thai Canteen': 'ChIJF7hkPnMphYAR9FL8d7Uficc', 'Pho Tasty': 'ChIJ8V3ETd0phYARqccVQfXrrlo', 'Cajun Feast': 'ChIJe9M5nKkphYARS6W4m9gjmXc', 'Crepeville': 'ChIJLY9oagsphYARWpjzMY0bqNc', 'Local Kitchens': 'ChIJRwNhKKgphYARU3K0ug7H4zg', \"Woodstock's Pizza\": 'ChIJ7S3xg3MphYAR5PDBO_23l-I', 'Uniboil': 'ChIJ-S2wbHMphYARKMsmrS5N9dI', 'Zen Toro Japanese Bistro & Sushi Bar': 'ChIJ92qFE3MphYAR6SoziozmNM4', 'Red 88 - Davis': 'ChIJO1IngXMphYARO402r5zkGt4', 'Nami Sushi': 'ChIJgbQYONQrhYARh7jk1nVjmK4', 'Tasty Kitchen': 'ChIJcz7g53QphYARM8aFW1HwxSI', 'Three Ladies Cafe': 'ChIJm7u1u3MphYARY1o7ImzdpYg', 'Jusco Japanese Restaurant': 'ChIJ4UvqSw4phYARoJC4YZbKG5s', 'Fire Wings - Davis': 'ChIJH6Tb-rYphYARDNYVPGPaqVA', \"Froggy's\": 'ChIJ3Zu2nnMphYAR0JmGBDsHTRc', 'Tres Hermanas': 'ChIJA0j8k3MphYARTwkKOK21vWY', 'Delta of Venus': 'ChIJn5AVdkAphYARn30NRW1rzEA', 'Manna Korean Restaurant': 'ChIJlcHil3QphYARXAdQPGVRyW0', 'I Love Sushi': 'ChIJfSV0cHcphYAR4k2c9QVDkCU', \"Raising Cane's Chicken Fingers\": 'ChIJ1Yn0BeMphYAR9n4Zc1bTY6w', \"Jack's Urban Eats\": 'ChIJE3DvycophYAR2M5xbWprIRo', \"Ike's Love & Sandwiches\": 'ChIJs6FoeXMphYAR-0E-8GOg24Y', 'Kathmandu Kitchen': 'ChIJcyP3h3MphYARLN9yBMu6_0M', 'Taqueria Guadalajara Grill - South Davis': 'ChIJ2_JY3bgrhYAR7C6wkywjnJg', 'Hunan Bar & Restaurant': 'ChIJEd8EMMnWmoARDW4_zFp12YU', 'Black Bear Diner - Davis': 'ChIJKdIZgwwphYARz__ktWk_ZU4', \"Sophia's Thai Bar & Kitchen\": 'ChIJE-m5F3MphYARRKJ-Y_YRlmw', 'Preethi Indian Cuisine': 'ChIJCXGynXMphYARIPafHdDz7FM', 'Symposium Restaurant & Pizza House': 'ChIJjUDAL50phYARTXiqwCVxMVA', 'Taqueria El Burrito': 'ChIJzxV3nXQphYARdIwu-B_qcGo', 'Dos Coyotes Border Cafe': 'ChIJdX32DmQphYARp9WOii6MZw0', \"Steve's Pizza\": 'ChIJ85hRjXQphYARN8itwty2hFg', 'Beach Hut Deli': 'ChIJSz5qF8grhYAR3ZOc6XwAeO8'}\n",
      "Reviews saved\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "import requests\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "key = \"AIzaSyBVuUp1tjagr2ghi3TXTgf3GnLIzRBxIDc\"  \n",
    "\n",
    "mod_double_dollar_restaurants = [\n",
    "    \"Bull 'N Mouth\",\n",
    "\"Yuchan Shokudo\",\n",
    "\"Paesanos\",\n",
    "\"Mamma Eatery\",\n",
    "\"Dumpling House\",\n",
    "\"Bruxie\",\n",
    "\"Burgers and Brew\",\n",
    "\"Cafe Bernardo \",\n",
    "\"Well Season\",\n",
    "\"Thai Canteen\",\n",
    "\"Pho Tasty\",\n",
    "\"Cajun Feast\",\n",
    "\"Crepeville\",\n",
    "\"Local Kitchens\",\n",
    "\"Woodstock's Pizza\",\n",
    "\"Uniboil\",\n",
    "\"Zen Toro Japanese Bistro & Sushi Bar\",\n",
    "\"Red 88 - Davis\",\n",
    "\"Nami Sushi\",\n",
    "\"Tasty Kitchen\",\n",
    "\"Three Ladies Cafe\",\n",
    "\"Jusco Japanese Restaurant\",\n",
    "\"Fire Wings - Davis\",\n",
    "\"Froggy's\",\n",
    "\"Tres Hermanas\",\n",
    "\"Delta of Venus\",\n",
    "\"Manna Korean Restaurant\",\n",
    "\"I Love Sushi\",\n",
    "\"Raising Cane's Chicken Fingers\",\n",
    "\"Jack's Urban Eats\",\n",
    "\"Ike's Love & Sandwiches\",\n",
    "\"Kathmandu Kitchen\",\n",
    "\"Taqueria Guadalajara Grill - South Davis\",\n",
    "\"Hunan Bar & Restaurant\",\n",
    "\"Black Bear Diner - Davis\",\n",
    "\"Sophia's Thai Bar & Kitchen\",\n",
    "\"Preethi Indian Cuisine\",\n",
    "\"Symposium Restaurant & Pizza House\",\n",
    "\"Taqueria El Burrito\",\n",
    "\"Dos Coyotes Border Cafe\",\n",
    "\"Steve's Pizza\",\n",
    "\"Beach Hut Deli\",\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "place_ids = {}\n",
    "\n",
    "\n",
    "for restaurant in mod_double_dollar_restaurants:\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json\"\n",
    "    params = {\n",
    "        \"input\": restaurant,\n",
    "        \"inputtype\": \"textquery\",\n",
    "        \"fields\": \"place_id\",\n",
    "        \"key\": key\n",
    "    }\n",
    "    response = requests.get(url, params=params).json()\n",
    "\n",
    "    if response.get(\"candidates\"):\n",
    "        place_id = response[\"candidates\"][0][\"place_id\"]\n",
    "        place_ids[restaurant] = place_id\n",
    "    else:\n",
    "        print(f\"No results found for {restaurant}\")\n",
    "\n",
    "print(\"Place IDs:\", place_ids)\n",
    "\n",
    "def clean_review(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  \n",
    "    words = word_tokenize(text)  \n",
    "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  \n",
    "    return \" \".join(cleaned_words)  \n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    \n",
    "    if polarity > 0:\n",
    "        sentiment = \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    \n",
    "    return polarity, sentiment\n",
    "\n",
    "def get_reviews(place_id):\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "    params = {\n",
    "        \"place_id\": place_id,\n",
    "        \"fields\": \"name,rating,reviews\",\n",
    "        \"key\": key\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.encoding = \"utf-8\"  \n",
    "    data = response.json()  \n",
    "\n",
    "    reviews_data = []\n",
    "    sentiment_counts = {\"Positive\": 0, \"Neutral\": 0, \"Negative\": 0}\n",
    "    \n",
    "    if \"result\" in data:\n",
    "        name = data[\"result\"].get(\"name\", \"N/A\")\n",
    "        overall_rating = data[\"result\"].get(\"rating\", \"N/A\")\n",
    "        reviews = data[\"result\"].get(\"reviews\", [])\n",
    "\n",
    "        for review in reviews:\n",
    "            raw_text = review.get(\"text\", \"No review text available\")\n",
    "            cleaned_text = clean_review(raw_text)\n",
    "            review_rating = review.get(\"rating\", \"N/A\")\n",
    "\n",
    "            \n",
    "            polarity, sentiment = analyze_sentiment(cleaned_text)\n",
    "            sentiment_counts[sentiment] += 1\n",
    "\n",
    "            reviews_data.append([name, overall_rating, cleaned_text, review_rating, polarity, sentiment])\n",
    "\n",
    "    \n",
    "    total_reviews = sum(sentiment_counts.values())\n",
    "    if total_reviews > 0:\n",
    "        positive_pct = (sentiment_counts[\"Positive\"] / total_reviews) * 100\n",
    "        neutral_pct = (sentiment_counts[\"Neutral\"] / total_reviews) * 100\n",
    "        negative_pct = (sentiment_counts[\"Negative\"] / total_reviews) * 100\n",
    "    else:\n",
    "        positive_pct = neutral_pct = negative_pct = 0\n",
    "\n",
    "    return reviews_data, positive_pct, neutral_pct, negative_pct\n",
    "\n",
    "\n",
    "with open(\"Google_mod_double_dollar_Sentiment.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8-sig\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Restaurant\", \"Overall Rating\", \"Cleaned Review\", \"Review Rating\", \"Sentiment Score\", \"Sentiment\", \n",
    "                     \"Positive %\", \"Neutral %\", \"Negative %\"])\n",
    "\n",
    "    for restaurant, place_id in place_ids.items():\n",
    "        reviews, pos_pct, neu_pct, neg_pct = get_reviews(place_id)\n",
    "        \n",
    "        for review in reviews:\n",
    "            writer.writerow(review + [pos_pct, neu_pct, neg_pct])\n",
    "\n",
    "print(\"Reviews saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a50b1f5c-76f5-4e3e-bd56-3b8618a47380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\danie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\danie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place IDs: {'Taqueria Guadalajara - North Davis': 'ChIJmcBpWrYphYARDm0Dtv5thYI', 'Village Bakery': 'ChIJe5Zil3MphYARQNLYrdf6BU0', 'Pachamama Coffee': 'ChIJISbKInMphYARjqTU2cWvGJg', 'Blaze Pizza': 'ChIJ83ZseXMphYARUGWBLYCF1pQ', 'The Posh Bagel': 'ChIJNYwhenMphYAR4zf63nyq-Lk', 'The Hotdogger': 'ChIJZeGPQHMphYAR-XDtr91EzV8', \"Zia's Delicatessen\": 'ChIJKSBbvXQphYAR_ome0fEwhcs', 'Icekrimski Cafe': 'ChIJI15CbnMphYAR4FZvsQboLQ8', 'Philz Coffee': 'ChIJ9fJhW3MphYARZAfKXjL2xXY', 'In-N-Out Burger': 'ChIJ46qybnIphYARtkwjkGo3I-g'}\n",
      "Reviews saved\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "import requests\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "key = \"AIzaSyBVuUp1tjagr2ghi3TXTgf3GnLIzRBxIDc\"  \n",
    "\n",
    "high_dollar_restaurants = [\n",
    "    \"Taqueria Guadalajara - North Davis\",\n",
    "\"Village Bakery\",\n",
    "\"Pachamama Coffee\",\n",
    "\"Blaze Pizza\",\n",
    "\"The Posh Bagel\",\n",
    "\"The Hotdogger\",\n",
    "\"Zia's Delicatessen\",\n",
    "\"Icekrimski Cafe\",\n",
    "\"Philz Coffee\",\n",
    "\"In-N-Out Burger\",\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "place_ids = {}\n",
    "\n",
    "for restaurant in high_dollar_restaurants:\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json\"\n",
    "    params = {\n",
    "        \"input\": restaurant,\n",
    "        \"inputtype\": \"textquery\",\n",
    "        \"fields\": \"place_id\",\n",
    "        \"key\": key\n",
    "    }\n",
    "    response = requests.get(url, params=params).json()\n",
    "\n",
    "    if response.get(\"candidates\"):\n",
    "        place_id = response[\"candidates\"][0][\"place_id\"]\n",
    "        place_ids[restaurant] = place_id\n",
    "    else:\n",
    "        print(f\"No results found for {restaurant}\")\n",
    "\n",
    "print(\"Place IDs:\", place_ids)\n",
    "\n",
    "def clean_review(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  \n",
    "    words = word_tokenize(text)  \n",
    "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  \n",
    "    return \" \".join(cleaned_words)  \n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    \n",
    "    if polarity > 0:\n",
    "        sentiment = \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    \n",
    "    return polarity, sentiment\n",
    "\n",
    "def get_reviews(place_id):\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "    params = {\n",
    "        \"place_id\": place_id,\n",
    "        \"fields\": \"name,rating,reviews\",\n",
    "        \"key\": key\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.encoding = \"utf-8\"  \n",
    "    data = response.json()  \n",
    "\n",
    "    reviews_data = []\n",
    "    sentiment_counts = {\"Positive\": 0, \"Neutral\": 0, \"Negative\": 0}\n",
    "    \n",
    "    if \"result\" in data:\n",
    "        name = data[\"result\"].get(\"name\", \"N/A\")\n",
    "        overall_rating = data[\"result\"].get(\"rating\", \"N/A\")\n",
    "        reviews = data[\"result\"].get(\"reviews\", [])\n",
    "\n",
    "        for review in reviews:\n",
    "            raw_text = review.get(\"text\", \"No review text available\")\n",
    "            cleaned_text = clean_review(raw_text)\n",
    "            review_rating = review.get(\"rating\", \"N/A\")\n",
    "\n",
    "            \n",
    "            polarity, sentiment = analyze_sentiment(cleaned_text)\n",
    "            sentiment_counts[sentiment] += 1\n",
    "\n",
    "            reviews_data.append([name, overall_rating, cleaned_text, review_rating, polarity, sentiment])\n",
    "\n",
    "    \n",
    "    total_reviews = sum(sentiment_counts.values())\n",
    "    if total_reviews > 0:\n",
    "        positive_pct = (sentiment_counts[\"Positive\"] / total_reviews) * 100\n",
    "        neutral_pct = (sentiment_counts[\"Neutral\"] / total_reviews) * 100\n",
    "        negative_pct = (sentiment_counts[\"Negative\"] / total_reviews) * 100\n",
    "    else:\n",
    "        positive_pct = neutral_pct = negative_pct = 0\n",
    "\n",
    "    return reviews_data, positive_pct, neutral_pct, negative_pct\n",
    "\n",
    "\n",
    "with open(\"Google_high_dollar_Sentiment.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8-sig\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Restaurant\", \"Overall Rating\", \"Cleaned Review\", \"Review Rating\", \"Sentiment Score\", \"Sentiment\", \n",
    "                     \"Positive %\", \"Neutral %\", \"Negative %\"])\n",
    "\n",
    "    for restaurant, place_id in place_ids.items():\n",
    "        reviews, pos_pct, neu_pct, neg_pct = get_reviews(place_id)\n",
    "        \n",
    "        for review in reviews:\n",
    "            writer.writerow(review + [pos_pct, neu_pct, neg_pct])\n",
    "\n",
    "print(\"Reviews saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e3fd2a7-2c9f-4ce4-893e-4ee29a57d345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\danie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\danie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\danie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place IDs: {'My Burma': 'ChIJQdea5KkphYARr5CVfMQOdbg', 'Yang Kee Dumpling': 'ChIJaY0DEhcphYARjTEXK2B8jqE', \"Sam's Mediterranean Cuisine\": 'ChIJVU0ioQsphYAR6So3YmB69ZI', 'Sit Lo Saigon': 'ChIJL4n23ykphYARDNNvxGh6v6w', 'Tasty Palace': 'ChIJEcjP02IphYARy0FHnHcT1Z0', 'Guads Tacos & Beer': 'ChIJg0OxGE8phYARV7tZrfqHe98', 'Mikuni': 'ChIJWe0S3XIphYARC_FlAsI8QJo', 'Good Friends Hawaiian Poke': 'ChIJ6fwlVnQphYARkFyrpLFN07Y', 'Four Seasons Gourmet Chinese Restaurant': 'ChIJSzKJkG8phYARF_cPzCHlIxQ', \"Tim's Kitchen\": 'ChIJMSvriSsphYARKT474ry8sjs', 'Tasty Gourmet': 'ChIJrX3v_2wrhYARdm4zWpF-XX0', 'T-Kumi Ramen & Rice Bowl': 'ChIJuQxzBBQphYARN9Auh_TQIPM', 'Huku Japanese Bistro': 'ChIJ1YdkkLgrhYARNkEJVh1F29I', 'Taqueria Davis': 'ChIJT9yzWnYphYARV18NZv93Yc0', 'Village Pizza & Pints': 'ChIJBaf_g5cphYAR7IKv3wq6RyA', 'Upper Crust Baking': 'ChIJb1QK_VwphYAR3UDujzmlkFw', 'Wok of Flame': 'ChIJ2_JY3bgrhYARv6f7pkIZtaM', 'Yeti Restaurant': 'ChIJL7BaX3MphYARGLMXuX1dfEs', 'Nick the Greek': 'ChIJmUZKc_ophYARGdJ4AMYUv0c', 'Zumapoke & Lush Ice': 'ChIJhXLfhnQphYARQZMXnmQA7Iw', \"Tommy J's Grill & Catering\": 'ChIJ3Zu2nnMphYARPXJtjiYdhu0', 'Paste Thai': 'ChIJ_QNx2K8rhYARX7lsUGewBLY', 'Tea List': 'ChIJezDUq3QphYAR-b9geQEe2Y8', 'Davis Food Co-op': 'ChIJIxr01p8phYAR8FDlyrQ3DQE', \"Shah's Halal Food - University Mall\": 'ChIJbTMAPUAphYAR-8jrNjCRi4k', 'Nugget Markets, 1414 E Covell Blvd, Davis, CA 95616': 'ChIJbViPb5AphYARTSpJ9qQdaJI', 'Lamppost Pizza': 'ChIJq21S-j8ohYARHJGIcQjWfO4', 'Nugget Markets, 409 Mace Blvd, Davis, CA 95618': 'ChIJb0xgkbgrhYARfn19EmZGX5c'}\n",
      "Reviews saved\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "import requests\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "key = \"AIzaSyBVuUp1tjagr2ghi3TXTgf3GnLIzRBxIDc\"  \n",
    "\n",
    "high_double_dollar_restaurants = [\n",
    "    \"My Burma\",\n",
    "\"Yang Kee Dumpling\",\n",
    "\"Sam's Mediterranean Cuisine\",\n",
    "\"Sit Lo Saigon\",\n",
    "\"Tasty Palace\",\n",
    "\"Guads Tacos & Beer\",\n",
    "\"Mikuni\",\n",
    "\"Good Friends Hawaiian Poke\",\n",
    "\"Four Seasons Gourmet Chinese Restaurant\",\n",
    "\"Tim's Kitchen\",\n",
    "\"Tasty Gourmet\",\n",
    "\"T-Kumi Ramen & Rice Bowl\",\n",
    "\"Huku Japanese Bistro\",\n",
    "\"Taqueria Davis\",\n",
    "\"Village Pizza & Pints\",\n",
    "\"Upper Crust Baking\",\n",
    "\"Wok of Flame\",\n",
    "\"Yeti Restaurant\",\n",
    "\"Nick the Greek\",\n",
    "\"Zumapoke & Lush Ice\",\n",
    "\"Tommy J's Grill & Catering\",\n",
    "\"Paste Thai\",\n",
    "\"Tea List\",\n",
    "\"Davis Food Co-op\",\n",
    "\"Shah's Halal Food - University Mall\",\n",
    "\"Nugget Markets, 1414 E Covell Blvd, Davis, CA 95616\",\n",
    "\"Lamppost Pizza\",\n",
    "\"Nugget Markets, 409 Mace Blvd, Davis, CA 95618\",\n",
    "]\n",
    "place_ids = {}\n",
    "\n",
    "for restaurant in high_double_dollar_restaurants:\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json\"\n",
    "    params = {\n",
    "        \"input\": restaurant,\n",
    "        \"inputtype\": \"textquery\",\n",
    "        \"fields\": \"place_id\",\n",
    "        \"key\": key\n",
    "    }\n",
    "    response = requests.get(url, params=params).json()\n",
    "\n",
    "    if response.get(\"candidates\"):\n",
    "        place_id = response[\"candidates\"][0][\"place_id\"]\n",
    "        place_ids[restaurant] = place_id\n",
    "    else:\n",
    "        print(f\"No results found for {restaurant}\")\n",
    "\n",
    "print(\"Place IDs:\", place_ids)\n",
    "\n",
    "def clean_review(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  \n",
    "    words = word_tokenize(text)  \n",
    "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  \n",
    "    return \" \".join(cleaned_words)  \n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    \n",
    "    if polarity > 0:\n",
    "        sentiment = \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    \n",
    "    return polarity, sentiment\n",
    "\n",
    "def get_reviews(place_id):\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "    params = {\n",
    "        \"place_id\": place_id,\n",
    "        \"fields\": \"name,rating,reviews\",\n",
    "        \"key\": key\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.encoding = \"utf-8\"  \n",
    "    data = response.json()  \n",
    "\n",
    "    reviews_data = []\n",
    "    sentiment_counts = {\"Positive\": 0, \"Neutral\": 0, \"Negative\": 0}\n",
    "    \n",
    "    if \"result\" in data:\n",
    "        name = data[\"result\"].get(\"name\", \"N/A\")\n",
    "        overall_rating = data[\"result\"].get(\"rating\", \"N/A\")\n",
    "        reviews = data[\"result\"].get(\"reviews\", [])\n",
    "\n",
    "        for review in reviews:\n",
    "            raw_text = review.get(\"text\", \"No review text available\")\n",
    "            cleaned_text = clean_review(raw_text)\n",
    "            review_rating = review.get(\"rating\", \"N/A\")\n",
    "\n",
    "        \n",
    "            polarity, sentiment = analyze_sentiment(cleaned_text)\n",
    "            sentiment_counts[sentiment] += 1\n",
    "\n",
    "            reviews_data.append([name, overall_rating, cleaned_text, review_rating, polarity, sentiment])\n",
    "\n",
    "\n",
    "    total_reviews = sum(sentiment_counts.values())\n",
    "    if total_reviews > 0:\n",
    "        positive_pct = (sentiment_counts[\"Positive\"] / total_reviews) * 100\n",
    "        neutral_pct = (sentiment_counts[\"Neutral\"] / total_reviews) * 100\n",
    "        negative_pct = (sentiment_counts[\"Negative\"] / total_reviews) * 100\n",
    "    else:\n",
    "        positive_pct = neutral_pct = negative_pct = 0\n",
    "\n",
    "    return reviews_data, positive_pct, neutral_pct, negative_pct\n",
    "\n",
    "\n",
    "with open(\"Google_high_double_dollar_Sentiment.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8-sig\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Restaurant\", \"Overall Rating\", \"Cleaned Review\", \"Review Rating\", \"Sentiment Score\", \"Sentiment\", \n",
    "                     \"Positive %\", \"Neutral %\", \"Negative %\"])\n",
    "\n",
    "    for restaurant, place_id in place_ids.items():\n",
    "        reviews, pos_pct, neu_pct, neg_pct = get_reviews(place_id)\n",
    "        \n",
    "        for review in reviews:\n",
    "            writer.writerow(review + [pos_pct, neu_pct, neg_pct])\n",
    "\n",
    "print(\"Reviews saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
